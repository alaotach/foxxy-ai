make the extension for chrome not firefox


LLM-Powered Chrome Automation Agent (Architecture Overview)

Content Script (Page Context): Injected into each web page, it can inspect and manipulate the DOM and simulate user-like events (clicks, typing, scrolling). For example, content scripts can call element.click() or element.scrollIntoView() to interact with the page. They communicate with the rest of the extension via chrome.runtime.sendMessage/onMessage.

Background Service Worker: In Manifest V3, the background runs as a service worker that persists independently of page loads. It acts as a central hub: receiving the user’s prompt (from a popup or content script), forwarding it to the backend LLM service, and relaying back the semantic actions. Because it is persistent, the background can maintain long-lived connections (e.g. WebSocket) to the backend. It also holds sensitive data like API keys, and has permissions (e.g. <all_urls>, “debugger”) to perform extension-wide actions.

Extension UI / Popup: A small HTML/JS interface (or context menu) that accepts natural-language prompts from the user. When the user submits a prompt, the UI sends it to the background script via chrome.runtime.sendMessage(), which then invokes the LLM backend.

LLM Backend (Node.js/Python Server): A separate server (e.g. Node.js with Express or Python FastAPI) that hosts the LLM integration. It receives the user’s prompt and any context (e.g. current URL or screenshot), calls the LLM API (OpenAI, Claude, etc.), and returns a structured JSON response: a sequence of semantic actions (e.g. {"action": "click", "selector": "#submit-button", "value": null}). Using a fixed JSON schema for actions ensures reliability. The backend can also enforce authentication and rate limits, and translate LLM output into a normalized format.

Communication Pattern

Extension ⇄ Background: The user’s prompt goes from the UI/popup to the background via chrome.runtime.sendMessage. The background then uses chrome.tabs.query/sendMessage or chrome.scripting.executeScript to dispatch commands to the content script of the active tab. Responses (e.g. action results or errors) flow back via the same message channels.

Background ⇄ LLM Backend: The background communicates with the external backend over HTTPS or WebSocket. For real-time control, a persistent WebSocket is ideal: the background (as a service worker) can maintain a WebSocket connection to the server and send prompts or interim page state, receiving action lists in return. WebSockets avoid the overhead of repeated HTTP handshakes. All traffic must be encrypted (WSS) and authenticated to protect user data.

Message Structure: Use concise JSON messages. For example, a prompt request might be { "prompt": "Log in to my account" }, and the LLM might reply with {"actions":[{"type":"navigate","url":"https://example.com/login"},{"type":"type","selector":"#user","text":"alice"},{"type":"type","selector":"#pass","text":"••••"},{"type":"click","selector":"#login"}]}. Structured JSON avoids brittle parsing of free-form text. Similarly, messages between extension parts can use small JSON with keys like {action:"clicked",status:"ok",detail:...}.

Secure Calls: The background should handle all network calls to LLM endpoints. This keeps API keys out of content scripts. If using OpenAI, for instance, the background can fetch the model and return results. (One pattern is to use chrome.scripting.executeScript to run code in the page context for DOM access, but then fetch from the background.)



Execution Flow

Single-Action Example:

User Prompt: The user enters “Click the signup button” in the extension UI.

LLM Interpretation: The background sends this prompt (plus minimal context like current URL) to the LLM backend. The LLM returns a single action, e.g. {"type":"click","selector":"button#signup","value":null}.

Action Execution: The background delivers this action to the content script (e.g. via chrome.tabs.sendMessage). The content script locates document.querySelector("button#signup"). It calls elem.scrollIntoView() and then elem.click() to simulate the click.

Post-Action Check: After clicking, the content script verifies success (e.g. a new element appears or URL changes). It sends a confirmation or error back to the background. Optionally, it takes a screenshot or logs the result for debugging.

Full Task Flow: For multi-step tasks (e.g. “Fill form and submit”), the system loops over multiple actions. There are two modes:

Plan-then-Execute (Batch): The LLM returns the entire action sequence in one JSON array. The extension executes steps 1…N in order. Between steps, it may capture logs or screenshots. If any step fails, it can abort or retry.

Iterative Interaction: Execute one or a few steps, then send updated state to the LLM for the next steps. For example, after step 1, the extension could take a snapshot of the page (DOM or screenshot) and include it in a follow-up prompt “Now after logging in, do X.” This allows the LLM to adapt to dynamic content. Tools like Stagehand use a loop where after each “think” or “act,” the agent can request the next action. In practice, a simple loop might be: send prompt, get actions, run them, then stop (batch) or restart prompt with “Done step 1” (iterative).


Reliability & Observability

Scroll and Visibility: Always bring target elements into view before interacting. Use element.scrollIntoView({behavior:'auto',block:'center'}) so off-screen elements become visible (similar to Selenium practices). This avoids “element not clickable” errors.

Explicit Waits: If an element is not immediately found or enabled, retry with delays. For example, poll document.querySelector() every 0.5s up to a timeout. Alternatively, use a MutationObserver to watch for dynamic content. Only proceed when the target element appears and is offsetParent !== null.

Stable Selectors: Prefer robust selectors (IDs, ARIA roles, unique class names). If a selector fails, fallback to alternative strategies: e.g. find by visible text (document.evaluate("//button[text()='Submit']")), use XPath, or query ARIA attributes. You can also try keyboard navigation (send keys via element.focus(); element.dispatchEvent(new KeyboardEvent('keydown',...))) if clicking fails.

Action Confirmation: After each action, verify its effect. For example, after a form submit, check if a success message appeared or a URL changed. If not, the script can retry the action or report an error. At a minimum, log the expected vs actual outcome.

Retries and Fallbacks: Implement a limit on retries per action (e.g. 3 attempts). If an action still fails, pause and notify the user rather than silently crashing. Possible fallbacks include trying an alternative selector, using element.click() vs. element.dispatchEvent(new MouseEvent('click')), or refreshing the page and re-running from a safe checkpoint.

Throttling and Delays: Introduce small delays (100–300ms) between actions to mimic human pace and let the page update. This also prevents overwhelming the page or triggering anti-bot measures.

Logging: Maintain a detailed log of each step: timestamp, action type, selector/text, and result. Write logs to chrome.storage.local or send them to a remote logging service for analysis. Logging aids debugging when things go wrong.

Screenshots: Capture the page state for observability. The extension can call chrome.tabs.captureVisibleTab() from the background to get a PNG of the current view. For full-page screenshots or deeper inspection, use chrome.debugger: attach to the tab and send the DevTools command Page.captureScreenshot via CDP. These images can be saved or sent to the backend for audit.

Auditing Actions: For security, record every decision: what the LLM suggested and what was executed. Stage logs or screenshots with overlays (e.g. highlight a clicked element) can be helpful. In a production system, you’d also implement “least privilege” (whitelisting allowed domains or actions) and mask any sensitive data passed to the LLM.


Chrome APIs & Enhancements

chrome.scripting.executeScript: Use this to programmatically inject and run JavaScript in the target tab. For example, to get page context or perform a fetch, you might call:

const [result] = await chrome.scripting.executeScript({
  target: { tabId: tab.id },
  func: () => { /* code running in page */ }
});


This is useful if you need to run code in the page’s isolated world (e.g. extract data or simulate user input).

chrome.runtime.onMessage: Listen for messages in both the background and content scripts. Use chrome.runtime.sendMessage() or chrome.tabs.sendMessage() to pass commands and results. This built-in messaging is secure and does not require opening ports.

chrome.debugger API: For advanced control, attach to a tab’s DevTools Protocol. With chrome.debugger.attach({tabId}, '1.3', callback) and chrome.debugger.sendCommand({tabId}, 'Page.captureScreenshot', {}, callback), you can capture full-page screenshots, intercept network traffic, or inject CSS at runtime. Remember to declare "debugger" permission.

chrome.tabs.captureVisibleTab: In simpler cases, this API captures a PNG of the visible viewport (requires “tabs” permission). As shown in example extensions, the content script can message the background to take a screenshot and return the data URI. You can then draw and crop it on a canvas if needed.

Service Worker Lifecycle: In MV3 the background is an event-driven service worker that may unload when idle. Use chrome.alarms or keep-alive messaging if needed to ensure completion of long tasks. Or simply trigger the flow on a user action (popup click) so the worker stays alive.

Permissions: Minimize requested permissions. Only use <all_urls> if necessary. For network requests, you may use the “host permissions” or the upcoming declarativeNetRequest for allowed domains. Use activeTab permission to temporarily inject scripts into the current tab.

UX Enhancements: Optionally use chrome.tabs.insertCSS to highlight elements or show a status overlay on the page during automation. This can help with debugging (showing which element was clicked). Also consider leveraging the [Chrome Prompt API] for on-device LLMs if offline support is desired (though typically we use a cloud LLM).


By following this plan – a clean separation of content/background/LLM, well-defined messaging, and robust execution safeguards (scrolling, waiting, logging) – even a small team can build a functional AI-driven browser assistant. The key is to treat each LLM output as semantic steps and carefully map them to concrete, reliable DOM interactions, while continuously monitoring and handling failures.

